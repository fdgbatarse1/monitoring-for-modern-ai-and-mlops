{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "QgumzOZ5wgec",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Model Monitoring with Evidently, MLFlow and Grafana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rByuPhg7wgei",
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow.tracking import MlflowClient\n",
    "from pathlib import Path\n",
    "from typing import Text, Any, Dict\n",
    "from sklearn import ensemble, model_selection\n",
    "\n",
    "from evidently.pipeline.column_mapping import ColumnMapping\n",
    "from evidently.report import Report\n",
    "from evidently.metrics import (\n",
    "    RegressionQualityMetric,\n",
    "    RegressionPredictedVsActualScatter,\n",
    "    RegressionPredictedVsActualPlot,\n",
    "    RegressionErrorPlot,\n",
    "    RegressionAbsPercentageErrorPlot,\n",
    "    RegressionErrorDistribution,\n",
    "    RegressionErrorNormality,  \n",
    ")\n",
    "\n",
    "import pendulum\n",
    "from sqlalchemy import Boolean, Column, Float, Integer, String\n",
    "from sqlalchemy.orm import declarative_base\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.orm import sessionmaker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HiiUl3p8wgej",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data \n",
    "DATA_DIR = \"data\"\n",
    "FILENAME = \"raw_data.csv\"\n",
    "REPORTS_DIR = 'reports'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "zw5Tap_Xwgej",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "VqGH1Mr6wgej",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "More information about the dataset can be found in UCI machine learning repository: https://archive.ics.uci.edu/ml/datasets/bike+sharing+dataset\n",
    "\n",
    "Acknowledgement: Fanaee-T, Hadi, and Gama, Joao, 'Event labeling combining ensemble detectors and background knowledge', Progress in Artificial Intelligence (2013): pp. 1-15, Springer Berlin Heidelberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dKX2YV19wgek",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Download original dataset with: python src/load_data.py \n",
    "raw_data = pd.read_csv(f\"../{DATA_DIR}/{FILENAME}\")\n",
    "\n",
    "# Set datetime index \n",
    "raw_data = raw_data.set_index('dteday')\n",
    "\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split for Batches (weeks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dates for train data\n",
    "train_dates = ('2011-01-02 00:00:00','2011-03-06 23:00:00')\n",
    "\n",
    "# Define dates for inference batches\n",
    "prediction_batches = [ \n",
    "    ('2011-03-07 00:00:00','2011-03-13 23:00:00'),\n",
    "    ('2011-03-14 00:00:00','2011-03-20 23:00:00'),\n",
    "    ('2011-03-21 00:00:00','2011-03-27 23:00:00'), \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define column mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'cnt'\n",
    "prediction = 'prediction'\n",
    "datetime = 'dteday'\n",
    "numerical_features = ['temp', 'atemp', 'hum', 'windspeed', 'mnth', 'hr', 'weekday']\n",
    "categorical_features = ['season', 'holiday', 'workingday', ]\n",
    "FEATURE_COLUMNS = numerical_features + categorical_features\n",
    "\n",
    "column_mapping = ColumnMapping()\n",
    "column_mapping.target = target\n",
    "column_mapping.prediction = prediction\n",
    "column_mapping.datetime = datetime\n",
    "column_mapping.numerical_features = numerical_features\n",
    "column_mapping.categorical_features = categorical_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = raw_data.loc['2011-01-01 00:00:00':'2011-01-28 23:00:00'].reset_index()\n",
    "\n",
    "print(sample_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
    "    sample_data[numerical_features + categorical_features],\n",
    "    sample_data[target],\n",
    "    test_size=0.3\n",
    ")\n",
    "\n",
    "regressor = ensemble.RandomForestRegressor(random_state = 0, n_estimators = 50)\n",
    "regressor.fit(X_train, y_train) \n",
    "\n",
    "regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = Path('../models/model.joblib')\n",
    "joblib.dump(regressor, model_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "2rJyfTJ3wger",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Design Monitoring Reports and Metrics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Evidently Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TZKZ_biHwgen",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define the reference dataset\n",
    "reference_data = raw_data.loc[train_dates[0]:train_dates[1]]\n",
    "reference_data['prediction'] = regressor.predict(reference_data[FEATURE_COLUMNS])\n",
    "reference_data = reference_data.reset_index(drop=True)\n",
    "\n",
    "print(reference_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TqljYICLwger",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "current_dates = prediction_batches[0]\n",
    "current_data = raw_data.loc[current_dates[0]:current_dates[1]]  \n",
    "\n",
    "print(current_data.shape)\n",
    "# current_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_prediction = regressor.predict(current_data[numerical_features + categorical_features])\n",
    "current_data['prediction'] = current_prediction\n",
    "current_data = current_data.reset_index(drop=True)\n",
    "\n",
    "print(current_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LvuPle_Nwges"
   },
   "outputs": [],
   "source": [
    "# Build the Model Monitoring report\n",
    "model_report = Report(metrics=[\n",
    "    RegressionQualityMetric(),\n",
    "    RegressionErrorPlot(),\n",
    "    RegressionErrorDistribution()\n",
    "])\n",
    "model_report.run(\n",
    "    reference_data=reference_data,\n",
    "    current_data=current_data,\n",
    "    column_mapping=column_mapping\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_report.show(mode='inline')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Monitoring Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_monitoring_metrics(\n",
    "    regression_quality_report: Report\n",
    ") -> Dict:\n",
    "\n",
    "    metrics = {} \n",
    "    report_dict = regression_quality_report.as_dict()\n",
    "    \n",
    "    metrics['me'] = report_dict['metrics'][0]['result']['current']['mean_error']\n",
    "    metrics['mae'] = report_dict['metrics'][0]['result']['current'][\"mean_abs_error\"]\n",
    "    metrics['rmse'] = report_dict['metrics'][0]['result']['current'][\"rmse\"]\n",
    "\n",
    "    # TODO: Uncomment for \"6. Add your own metrics\"\n",
    "    # metrics['mape'] = report_dict['metrics'][0]['result']['current'][\"mean_abs_perc_error\"] \n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_metrics = get_model_monitoring_metrics(model_report)\n",
    "model_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare monitoring database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USER = \"admin\"\n",
    "PASSWORD = \"admin\"\n",
    "\n",
    "MONITORING_DB_URI = f\"postgresql+psycopg2://{USER}:{PASSWORD}@127.0.0.1:5432/monitoring_db\"\n",
    "MONITORING_DB_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new base model\n",
    "Base = declarative_base()\n",
    "\n",
    "class ModelPerformanceTable(Base):\n",
    "    \"\"\"Implement table for model performance metrics.\"\"\"\n",
    "\n",
    "    __tablename__ = \"model_performance\"\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    timestamp = Column(Float)\n",
    "    me_default_sigma = Column(Float)\n",
    "    mean_abs_error_default = Column(Float)\n",
    "    rmse_default = Column(Float)\n",
    "\n",
    "    # TODO: Uncomment for \"6. Add your own metrics\"\n",
    "    # mean_abs_perc_error_default = Column(Float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_db(monitoring_db_uri):\n",
    "    engine = create_engine(monitoring_db_uri)\n",
    "    Base.metadata.create_all(engine)\n",
    "    print(\"Database created successfully\")\n",
    "\n",
    "def drop_db(monitoring_db_uri):\n",
    "    engine = create_engine(monitoring_db_uri)\n",
    "    Base.metadata.drop_all(engine)\n",
    "    print(\"Database dropped successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean database from previous run\n",
    "drop_db(MONITORING_DB_URI)\n",
    "\n",
    "# Build monitoring database structure\n",
    "create_db(MONITORING_DB_URI)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "q5lW24Xzwgex",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Run Model Quality Monitoring (weekly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create SQLAlchemy session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I_IyYlM0wgey",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create SQLAlchemy engine object\n",
    "sqa_engine = create_engine(MONITORING_DB_URI)\n",
    "\n",
    "# Get Session class\n",
    "Session = sessionmaker(bind=sqa_engine)\n",
    "\n",
    "# Create SQLAlchemy session\n",
    "sqa_session = Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate and log metrics to PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bTHU8eAqwgez",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Run model monitoring for each batch of dates\n",
    "for current_dates in prediction_batches:\n",
    "\n",
    "    batch_start = current_dates[0]\n",
    "    batch_end = current_dates[1]\n",
    "    print(f\"Current batch start: {batch_start}\") \n",
    "    print(f\"Current batch end: {batch_end}\\n\") \n",
    "    \n",
    "    # Make predictions for the current batch data\n",
    "    current_data = raw_data.loc[batch_start:batch_end]\n",
    "    current_prediction = regressor.predict(current_data[FEATURE_COLUMNS])\n",
    "    current_data['prediction'] = current_prediction\n",
    "    current_data = current_data.reset_index(drop=True)\n",
    "\n",
    "    # Build the Model Monitoring report\n",
    "    model_report = Report(metrics=[\n",
    "        RegressionQualityMetric(),\n",
    "        RegressionErrorPlot(),\n",
    "        RegressionErrorDistribution()\n",
    "    ])\n",
    "    model_report.run(\n",
    "        reference_data=reference_data,\n",
    "        current_data=current_data,\n",
    "        column_mapping=column_mapping\n",
    "    )\n",
    "    \n",
    "    # Log Metrics\n",
    "    model_metrics = get_model_monitoring_metrics(model_report)\n",
    "\n",
    "    # Create new model performance record\n",
    "    timestamp = pendulum.parse(batch_end).timestamp()\n",
    "    model_performance = ModelPerformanceTable(\n",
    "        timestamp=timestamp,\n",
    "        me_default_sigma=model_metrics[\"me\"],\n",
    "        mean_abs_error_default=model_metrics[\"mae\"],\n",
    "        rmse_default=model_metrics[\"rmse\"],\n",
    "\n",
    "        # TODO: Uncomment for \"6. Add your own metrics\"\n",
    "        # mean_abs_perc_error_default=model_metrics[\"mape\"],\n",
    "    )\n",
    "    # Add and commit the new record to the database\n",
    "    sqa_session.add(model_performance)\n",
    "    sqa_session.commit()\n",
    "\n",
    "# Close SQLAlchemy session\n",
    "sqa_session.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Add your own metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:**\n",
    "\n",
    "1.  Calculate a new metric\n",
    "2.  Add metrics to the DB table scheme\n",
    "3.  Log metrics to DB\n",
    "4.  Add/update Panel/Dashboard\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example: add `MAPE` metric**\n",
    "1.  Calculate a new metric:\n",
    "      - Uncomment & run `3.2 Calculate Monitoring Metrics` \n",
    "3.  Add metrics to the DB table scheme:\n",
    "      - Uncomment & run `4. Prepare monitoring database` \n",
    "4.  Log metrics to DB:\n",
    "      - Run cells in `5. Run Model Quality Monitoring` \n",
    "5.  Add/update Panel/Dashboard: Update Grafana dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
